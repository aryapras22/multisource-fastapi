{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Scraper with Advanced Content Cleaning\n",
        "\n",
        "This notebook extracts news articles and applies comprehensive cleaning to get only the article content.\n",
        "\n",
        "## Requirements\n",
        "```bash\n",
        "pip install newspaper4k pygooglenews lxml_html_clean googlenewsdecoder\n",
        "```\n",
        "\n",
        "## Cleaning Features\n",
        "- Remove emails and contact information\n",
        "- Remove social media links and CTAs (call-to-action)\n",
        "- Remove navigation text and headers\n",
        "- Remove advertisements and boilerplate\n",
        "- Clean and format into proper paragraphs\n",
        "- Remove excessive whitespace\n",
        "- Filter out short non-content sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pygooglenews import GoogleNews\n",
        "from newspaper import Article\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "from googlenewsdecoder import new_decoderv1\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_article_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive cleaning of article text to remove boilerplate, ads, and noise\n",
        "    Returns clean paragraphs of article content only\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    # Step 1: Remove email addresses\n",
        "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
        "    \n",
        "    # Step 2: Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    \n",
        "    # Step 3: Remove LaTeX patterns\n",
        "    text = re.sub(r'\\$.*?\\$', '', text)  # Inline math\n",
        "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text, flags=re.DOTALL)  # Display math\n",
        "    text = re.sub(r'\\\\\\[.*?\\\\\\]', '', text, flags=re.DOTALL)  # Display math\n",
        "    text = re.sub(r'\\\\\\(.*?\\\\\\)', '', text, flags=re.DOTALL)  # Display math\n",
        "    text = re.sub(r'\\\\begin\\{[a-z]+\\*?\\}.*?\\\\end\\{[a-z]+\\*?\\}', '', text, flags=re.DOTALL) # Environments\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?(?:\\{[^\\}]*\\})?', '', text) # Commands\n",
        "    \n",
        "    # Step 4: Remove common boilerplate patterns\n",
        "    boilerplate_patterns = [\n",
        "        r'(?i)subscribe to our newsletter',\n",
        "        r'(?i)sign up for our newsletter',\n",
        "        r'(?i)follow us on',\n",
        "        r'(?i)share this article',\n",
        "        r'(?i)read more:',\n",
        "        r'(?i)advertisement',\n",
        "        r'(?i)click here',\n",
        "        r'(?i)related articles',\n",
        "        r'(?i)you may also like',\n",
        "        r'(?i)recommended for you',\n",
        "        r'(?i)terms of service',\n",
        "        r'(?i)privacy policy',\n",
        "        r'(?i)cookie policy',\n",
        "        r'(?i)all rights reserved',\n",
        "        r'(?i)copyright ©',\n",
        "        r'©\\s*\\d{4}',\n",
        "        r'(?i)join our community',\n",
        "        r'(?i)get the latest',\n",
        "        r'(?i)breaking news',\n",
        "        r'(?i)trending now',\n",
        "    ]\n",
        "    \n",
        "    for pattern in boilerplate_patterns:\n",
        "        text = re.sub(pattern + r'[^.!?]*[.!?]', '', text)\n",
        "    \n",
        "    # Step 5: Split into sentences and filter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    \n",
        "    # Filter out short sentences (likely navigation/ads)\n",
        "    # Keep sentences with at least 10 words\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        \n",
        "        # Skip if too short\n",
        "        word_count = len(sentence.split())\n",
        "        if word_count < 10:\n",
        "            continue\n",
        "        \n",
        "        # Skip if contains too many capital letters (likely navigation)\n",
        "        capitals = sum(1 for c in sentence if c.isupper())\n",
        "        if len(sentence) > 0 and capitals / len(sentence) > 0.3:\n",
        "            continue\n",
        "        \n",
        "        # Skip sentences with common navigation patterns\n",
        "        nav_keywords = ['facebook', 'twitter', 'instagram', 'linkedin', 'youtube', \n",
        "                       'subscribe', 'newsletter', 'advertisement', 'sponsored']\n",
        "        if any(keyword in sentence.lower() for keyword in nav_keywords):\n",
        "            continue\n",
        "        \n",
        "        cleaned_sentences.append(sentence)\n",
        "    \n",
        "    # Step 6: Remove excessive punctuation and special characters\n",
        "    cleaned_text = ' '.join(cleaned_sentences)\n",
        "    cleaned_text = re.sub(r'[^\\w\\s.,!?;:\\'\\\"\\-()]', ' ', cleaned_text)\n",
        "    \n",
        "    # Step 7: Remove excessive whitespace (this includes newlines like \\n)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    \n",
        "    # Step 8: Format into paragraphs (split long text every 3-5 sentences)\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', cleaned_text)\n",
        "    paragraphs = []\n",
        "    current_paragraph = []\n",
        "    \n",
        "    for i, sentence in enumerate(sentences):\n",
        "        current_paragraph.append(sentence)\n",
        "        \n",
        "        # Create a new paragraph every 4-5 sentences\n",
        "        if (i + 1) % 4 == 0 and len(current_paragraph) > 0:\n",
        "            paragraphs.append(' '.join(current_paragraph))\n",
        "            current_paragraph = []\n",
        "    \n",
        "    # Add remaining sentences\n",
        "    if current_paragraph:\n",
        "        paragraphs.append(' '.join(current_paragraph))\n",
        "    \n",
        "    # Join paragraphs with double line breaks\n",
        "    final_text = '\\n\\n'.join(paragraphs)\n",
        "    \n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_google_news_url(google_url, max_retries=3):\n",
        "    \"\"\"\n",
        "    Decode Google News URL to get the actual article URL\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = new_decoderv1(google_url, interval=2)\n",
        "            \n",
        "            if result.get('status'):\n",
        "                decoded_url = result.get('decoded_url')\n",
        "                if decoded_url and 'http' in decoded_url:\n",
        "                    return decoded_url\n",
        "            \n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "        \n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(f\"   Error decoding URL: {str(e)}\")\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_full_article(url):\n",
        "    \"\"\"\n",
        "    Extract and clean full article content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        original_url = url\n",
        "        \n",
        "        # Decode Google News URL if needed\n",
        "        if 'news.google.com' in url:\n",
        "            print(f\"   Decoding Google News URL...\")\n",
        "            url = decode_google_news_url(url)\n",
        "            \n",
        "            if not url:\n",
        "                return {\n",
        "                    'url': original_url,\n",
        "                    'resolved_url': None,\n",
        "                    'title': 'Error',\n",
        "                    'authors': '',\n",
        "                    'publish_date': None,\n",
        "                    'raw_content': '',\n",
        "                    'cleaned_content': '',\n",
        "                    'top_image': '',\n",
        "                    'keywords': '',\n",
        "                    'raw_length': 0,\n",
        "                    'cleaned_length': 0,\n",
        "                    'extraction_status': 'Failed: Could not decode Google News URL'\n",
        "                }\n",
        "            \n",
        "            print(f\"   ✓ Decoded to: {url[:80]}...\")\n",
        "        \n",
        "        # Extract article content\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        \n",
        "        # Extract NLP features\n",
        "        try:\n",
        "            article.nlp()\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        raw_text = article.text\n",
        "        \n",
        "        # Apply comprehensive cleaning\n",
        "        print(f\"   Cleaning content...\")\n",
        "        cleaned_text = clean_article_text(raw_text)\n",
        "        \n",
        "        return {\n",
        "            'url': original_url,\n",
        "            'resolved_url': url,\n",
        "            'title': article.title,\n",
        "            'authors': ', '.join(article.authors) if article.authors else 'Unknown',\n",
        "            'publish_date': str(article.publish_date) if article.publish_date else None,\n",
        "            'raw_content': raw_text,\n",
        "            'cleaned_content': cleaned_text,\n",
        "            'top_image': article.top_image,\n",
        "            'keywords': ', '.join(article.keywords) if hasattr(article, 'keywords') and article.keywords else '',\n",
        "            'raw_length': len(raw_text),\n",
        "            'cleaned_length': len(cleaned_text),\n",
        "            'extraction_status': 'Success'\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'url': original_url if 'original_url' in locals() else url,\n",
        "            'resolved_url': url if 'original_url' in locals() else None,\n",
        "            'title': 'Error',\n",
        "            'authors': '',\n",
        "            'publish_date': None,\n",
        "            'raw_content': '',\n",
        "            'cleaned_content': '',\n",
        "            'top_image': '',\n",
        "            'keywords': '',\n",
        "            'raw_length': 0,\n",
        "            'cleaned_length': 0,\n",
        "            'extraction_status': f'Failed: {str(e)}'\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_news(query=None, topic=None, max_results=10):\n",
        "    \"\"\"\n",
        "    Search for news articles using query OR topic\n",
        "    \"\"\"\n",
        "    gn = GoogleNews(lang='en', country='US')\n",
        "    \n",
        "    # Determine search mode\n",
        "    if query and topic:\n",
        "        print(f\"⚠️  WARNING: PyGoogleNews cannot combine query + topic filters.\")\n",
        "        print(f\"   Using QUERY ONLY: '{query}' (ignoring topic filter)\\n\")\n",
        "        search_result = gn.search(query)\n",
        "        search_mode = f\"Query: '{query}' (across all topics)\"\n",
        "    elif query:\n",
        "        print(f\"Searching for: '{query}' (across all topics)\")\n",
        "        search_result = gn.search(query)\n",
        "        search_mode = f\"Query: '{query}'\"\n",
        "    elif topic:\n",
        "        print(f\"Getting latest headlines for topic: {topic}\")\n",
        "        search_result = gn.topic_headlines(topic)\n",
        "        search_mode = f\"Topic: {topic}\"\n",
        "    else:\n",
        "        print(\"Getting top news stories\")\n",
        "        search_result = gn.top_news()\n",
        "        search_mode = \"Top News\"\n",
        "    \n",
        "    print(f\"Max results: {max_results}\\n\")\n",
        "    \n",
        "    articles = []\n",
        "    entries = search_result.get('entries', [])[:max_results]\n",
        "    \n",
        "    print(f\"Found {len(entries)} articles. Extracting and cleaning content...\\n\")\n",
        "    \n",
        "    for idx, entry in enumerate(entries, 1):\n",
        "        url = entry.link\n",
        "        print(f\"[{idx}/{len(entries)}] Processing: {entry.title[:60]}...\")\n",
        "        \n",
        "        article_data = extract_full_article(url)\n",
        "        article_data['search_query'] = query if query else 'N/A'\n",
        "        article_data['topic'] = topic if topic else 'N/A'\n",
        "        article_data['search_mode'] = search_mode\n",
        "        articles.append(article_data)\n",
        "        \n",
        "        if article_data['extraction_status'] == 'Success':\n",
        "            print(f\"   ✓ Raw: {article_data['raw_length']:,} chars | Cleaned: {article_data['cleaned_length']:,} chars\")\n",
        "            print(f\"   Removed: {article_data['raw_length'] - article_data['cleaned_length']:,} chars of noise\\n\")\n",
        "        else:\n",
        "            print(f\"   ✗ {article_data['extraction_status']}\\n\")\n",
        "        \n",
        "        time.sleep(1)\n",
        "    \n",
        "    return articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search and Extract Articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Searching for: 'Parking occupancy monitoring software' (across all topics)\n",
            "Max results: 5\n",
            "\n",
            "Found 5 articles. Extracting and cleaning content...\n",
            "\n",
            "[1/5] Processing: A digital twin framework for urban parking management and mo...\n",
            "   Decoding Google News URL...\n",
            "   ✓ Decoded to: https://www.nature.com/articles/s41467-025-65306-w...\n",
            "   Cleaning content...\n",
            "   ✓ Raw: 77,191 chars | Cleaned: 74,278 chars\n",
            "   Removed: 2,913 chars of noise\n",
            "\n",
            "[2/5] Processing: Europe Parking Management Market Size, Share & Growth, 2033 ...\n",
            "   Decoding Google News URL...\n",
            "   ✓ Decoded to: https://www.marketdataforecast.com/market-reports/europe-parking-management-mark...\n",
            "   Cleaning content...\n",
            "   ✓ Raw: 20,735 chars | Cleaned: 20,464 chars\n",
            "   Removed: 271 chars of noise\n",
            "\n",
            "[3/5] Processing: 5 ways police departments are using RTCCs beyond crime fight...\n",
            "   Decoding Google News URL...\n",
            "   ✓ Decoded to: https://www.police1.com/real-time-crime-center/5-ways-police-departments-are-usi...\n",
            "   Cleaning content...\n",
            "   ✓ Raw: 9,255 chars | Cleaned: 9,108 chars\n",
            "   Removed: 147 chars of noise\n",
            "\n",
            "[4/5] Processing: Parking Reservation System Market Size - Global Market Insig...\n",
            "   Decoding Google News URL...\n",
            "   ✓ Decoded to: https://www.gminsights.com/industry-analysis/parking-reservation-system-market...\n",
            "   Cleaning content...\n",
            "   ✓ Raw: 32,760 chars | Cleaned: 32,613 chars\n",
            "   Removed: 147 chars of noise\n",
            "\n",
            "[5/5] Processing: Parking Guidance Systems Announces Global Merger With INDECT...\n",
            "   Decoding Google News URL...\n",
            "   ✓ Decoded to: https://www.businesswire.com/news/home/20250908581978/en/Parking-Guidance-System...\n",
            "   Cleaning content...\n",
            "   ✓ Raw: 4,012 chars | Cleaned: 3,824 chars\n",
            "   Removed: 188 chars of noise\n",
            "\n",
            "\n",
            "================================================================================\n",
            "✓ Successfully extracted 5 out of 5 articles\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>authors</th>\n",
              "      <th>raw_length</th>\n",
              "      <th>cleaned_length</th>\n",
              "      <th>extraction_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A digital twin framework for urban parking man...</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>77191</td>\n",
              "      <td>74278</td>\n",
              "      <td>Success</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Europe Parking Management Market Size, Share &amp;...</td>\n",
              "      <td>Market Data Forecast, Market Data Forecast ltd</td>\n",
              "      <td>20735</td>\n",
              "      <td>20464</td>\n",
              "      <td>Success</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5 ways police departments are using RTCCs beyo...</td>\n",
              "      <td>Sarah Calams, Sarah Calams Sarah Calams, Sarah...</td>\n",
              "      <td>9255</td>\n",
              "      <td>9108</td>\n",
              "      <td>Success</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parking Reservation System Market Size</td>\n",
              "      <td>Satyam Jaiswal, Preeti Wadhwani</td>\n",
              "      <td>32760</td>\n",
              "      <td>32613</td>\n",
              "      <td>Success</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Parking Guidance Systems Announces Global Merg...</td>\n",
              "      <td>Felicia Perez</td>\n",
              "      <td>4012</td>\n",
              "      <td>3824</td>\n",
              "      <td>Success</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  A digital twin framework for urban parking man...   \n",
              "1  Europe Parking Management Market Size, Share &...   \n",
              "2  5 ways police departments are using RTCCs beyo...   \n",
              "3             Parking Reservation System Market Size   \n",
              "4  Parking Guidance Systems Announces Global Merg...   \n",
              "\n",
              "                                             authors  raw_length  \\\n",
              "0                                            Unknown       77191   \n",
              "1     Market Data Forecast, Market Data Forecast ltd       20735   \n",
              "2  Sarah Calams, Sarah Calams Sarah Calams, Sarah...        9255   \n",
              "3                    Satyam Jaiswal, Preeti Wadhwani       32760   \n",
              "4                                      Felicia Perez        4012   \n",
              "\n",
              "   cleaned_length extraction_status  \n",
              "0           74278           Success  \n",
              "1           20464           Success  \n",
              "2            9108           Success  \n",
              "3           32613           Success  \n",
              "4            3824           Success  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# INPUT YOUR SEARCH PARAMETERS HERE\n",
        "query = \"Parking occupancy monitoring software\"  # Your search query\n",
        "max_results = 5  # Number of articles\n",
        "\n",
        "# Search and extract\n",
        "articles = search_news(query=query, max_results=max_results)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(articles)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"✓ Successfully extracted {len(df[df['extraction_status'] == 'Success'])} out of {len(df)} articles\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "df[['title', 'authors', 'raw_length', 'cleaned_length', 'extraction_status']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Cleaned Article (Before & After)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TITLE: A digital twin framework for urban parking management and mobility forecasting\n",
            "AUTHOR(S): Unknown\n",
            "PUBLISHED: 2025-10-23 00:00:00\n",
            "URL: https://www.nature.com/articles/s41467-025-65306-w\n",
            "================================================================================\n",
            "\n",
            "RAW CONTENT LENGTH: 77,191 characters\n",
            "CLEANED CONTENT LENGTH: 74,278 characters\n",
            "REMOVED: 2,913 characters (3.8% noise)\n",
            "\n",
            "================================================================================\n",
            "BEFORE CLEANING (First 500 chars):\n",
            "================================================================================\n",
            "Statistical outputs and distributions\n",
            "\n",
            "This section presents the key findings from the three components of our DT framework. Statistical analysis has played a crucial role in understanding urban mobility dynamics, providing a detailed insight into parking resource usage and user behavior. By examining transaction distributions, parking occupancy rates, and recorded violations, recurring patterns and specific issues within the analyzed urban context were identified. See Fig. 2d, e for weekday vs....\n",
            "\n",
            "================================================================================\n",
            "AFTER CLEANING (Full cleaned article in paragraphs):\n",
            "================================================================================\n",
            "Statistical outputs and distributions This section presents the key findings from the three components of our DT framework. Statistical analysis has played a crucial role in understanding urban mobility dynamics, providing a detailed insight into parking resource usage and user behavior. By examining transaction distributions, parking occupancy rates, and recorded violations, recurring patterns and specific issues within the analyzed urban context were identified. On weekdays, transactions are more frequent and exhibit greater variability, with peaks between 10 12 A.M.\n",
            "\n",
            "and 4 6 P.M., corresponding to work-related commuting and daily activities. In contrast, holiday usage is more uniform, characterized by a gradual midday increase and an even evening distribution. The city of Caserta is divided into two distinct areas: the central and peripheral areas, each characterized by different parking fees. These areas are further subdivided into smaller zones, which facilitates the allocation of traffic wardens, henceforth referred to as agents for simplicity, to specific locations.\n",
            "\n",
            "This structure also enables an analysis of citizens behavior based on particular geographical areas. Indeed, the analysis of parking stall occupancy revealed considerable spatial and temporal variation (refer to Fig. A clear distinction emerged not only between weekdays and weekends but also across different urban areas. Specifically, the central zone exhibited higher utilization frequencies, with a critical threshold at the 75th percentile around 0.6, compared to the peripheral zone.\n",
            "\n",
            "This disparity is statistically associated with the spatial distribution of offices, commercial establishments, and tourist attractions, and is further supported by Granger-causality tests confirming temporal precedence of activity density over occupancy rates. Both zones showed reduced occupancy on weekends compared to weekdays, although the decline was more pronounced in peripheral areas, likely due to their decreased appeal during non-working hours. In contrast, while the central zone experienced a slight decrease, it maintained relatively high occupancy levels on weekends, particularly near leisure destinations such as restaurants, shops, and cultural venues. External factors, such as weather conditions, air quality, points of interest, and local events, play a significant role in influencing parking usage and transaction patterns.\n",
            "\n",
            "In our initial analysis, we explored potential correlations between parking meter transactions and air pollution levels to understand how traffic flow might impact air quality (see Fig. 3c for correlation heatmap between transactions and air quality variables). The next step involved analyzing the relationship between weather conditions and parking meter usage. 3a, b for two common effects of adverse weather on the city.\n",
            "\n",
            "3b demonstrates a significant increase in parking demand in high-traffic areas such as near schools, hospitals, and offices, while Fig. 3a illustrates a decrease in parking demand in residential areas, where citizen services are less numerous, a finding quantitatively supported by correlation coefficients and further validated through what-if simulations that replicate this shift under multiple rainy-day scenarios. The analysis then moved on to study data related to city events. The graph shows a significant increase in the number of transactions (cumulative, across all parking meters in the urban area adjacent to event locations) on event days, with varying degrees of increase depending on the type of event.\n",
            "\n",
            "Note that each event type is displayed in a separate time window corresponding to its actual occurrence period. This explains the use of different y-axis scales across the highlighted segments. An additional analysis examined the relationship between total parking occupancy and the percentage of unauthorized parking. The study found that unauthorized parking rates varied significantly across different zones, with percentages ranging from 24.6 to 35.5 (See Fig.\n",
            "\n",
            "This result suggests that areas with higher parking demand tend to experience a great portion of unauthorized parking. While this pattern is somewhat predictable, it serves as an essential indicator for identifying areas with elevated parking challenges. Furthermore, an analysis explored the relationship between the rate of unauthorized parking and the issuance of fines. The Mann Kendall test revealed an inverse relationship between unauthorized parking trends and fine issuance.\n",
            "\n",
            "Specifically, in zones and time slots where unauthorized parking prevalence increased, a decrease in fine issuance was observed, and vice versa. Figure 2h illustrates a general pattern in which zones with higher levels of unauthorized parking tended to have fewer fines issued, especially during the morning and evening hours. Although a similar trend was observed in the afternoon, the concentration of data points at lower unauthorized parking indices was less pronounced. These findings underscore the dynamic interplay between monitoring efforts and compliance behaviors, offering a data-driven foundation for optimizing enforcement strategies and resource allocation.\n",
            "\n",
            "2g shows a negative correlation between the number of fines issued and the unauthorized parking rate, further supporting the observed inverse relationship. The findings from this analysis have significantly impacted the understanding of mobility dynamics. These results make it possible to design targeted interventions and envision more sustainable urban mobility management policies, ultimately benefiting both users and the city as a whole. Automated scheduling of traffic wardens Agents, responsible for ensuring compliance with parking regulations, play a critical role in maintaining order in urban areas.\n",
            "\n",
            "Automating their shift schedules presents a multifaceted challenge, requiring careful consideration of operational logistics, performance metrics, and equitable workload distribution. Our primary objective is to ensure an equitable distribution of the agents on a daily, weekly, and zone-specific basis to guarantee efficient and effective coverage of the areas subject to monitoring. This automation system was developed based on a comprehensive analysis of some Key Performance Indicators (KPIs) of the agents, detailed in the related section. These KPIs encompass critical metrics such as zone coverage, adherence to contractual working hours, and the activities performed during shifts.\n",
            "\n",
            "Additionally, a thorough analysis of peak parking occupancy periods and violation distributions was conducted to identify the most critical times and areas requiring attention. Leveraging these data insights, we have designed a tailored automation plan aimed at satisfying operational constraints and maximizing the effectiveness of patrolling activities. The algorithm we developed is capable of automatically generating a weekly shift schedule for agents. 4e for an example of a generated weekly shift schedule.\n",
            "\n",
            "The generated schedule strictly adheres to the provisions outlined in the labor contracts of each agent, ensuring that shifts are assigned in compliance with the specified weekly working hours, the designated number of workdays and rest days, and the maximum allowable working hours per day. The system also promotes an equitable distribution of workload among agents, preventing overburdening while maintaining operational efficiency. Additionally, it guarantees continuous patrolling of zones during active paid parking hours, with particular attention to peak usage periods and time slots characterized by higher violation rates. This automated scheduling process adopts a dynamic and adaptable approach, enabling swift adjustments to shifts in response to variations in parking occupancy patterns or exceptional circumstances.\n",
            "\n",
            "Predictive analysis of parking meter transactions, revenues, and street occupancy rates The predictive analysis of parking meter transactions, revenue, and street occupancy rates has been a crucial component in developing more efficient, data-driven urban mobility management strategies. By leveraging ML and DL models, we identified hidden patterns and forecasted future behaviors, providing valuable insights for resource planning. Unlike basic descriptive statistics, the adoption of predictive models allowed us to project the effects of current conditions into the future and estimate the evolution of key variables based on complex factors that are difficult to isolate without advanced analytical approaches. A crucial factor that enhanced the quality and accuracy of the predictions was the integration, through data fusion, of a wide variety of data from heterogeneous sources.\n",
            "\n",
            "In addition to operational data from parking meters and historical parking occupancy records, we incorporated contextual data such as Points of Interest (POIs), e.g., shops, restaurants, public offices, and tourist attractions, and information about scheduled events like fairs, concerts, and weekly markets. Detailed descriptions of these POIs and the events calendar are provided in the section Methods. This comprehensive dataset enabled the model to identify and analyze correlations between patterns of human activity and the utilization of parking infrastructure. The predictions underscored the importance of considering the location of POIs and events for more effective mobility management.\n",
            "\n",
            "For instance, proximity to tourist attractions showed a significant impact on occupancy rates, with substantial variations even across short distances. Similarly, temporary events, such as a concert in a central square, created spikes in demand during specific time slots and areas of the city, causing ripple effects in adjacent zones. Moreover, the predictive models revealed strong dependencies not only on the geolocation of POIs and events but also on temporal variables such as the day of the week and the time of day, as well as weather conditions. These additional factors provided a deeper understanding of citizens behavior, highlighting how occupancy rates and transaction volumes are influenced by temporal patterns and weather changes.\n",
            "\n",
            "To quantitatively evaluate the predictive performance of our models, we report key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2) across all analyzed zones. Tables S5 and S6 in Supplementary Materials present these values for three target variables: transaction count, transaction amount, and street occupancy rate. On average, the model achieves the lowest error metrics for the amount prediction (MSE 3.87e 03, MAE 3.52e 02, RMSE 5.91e 02), while street occupancy rate and transaction count predictions show balanced performance across metrics. The results highlight consistent and satisfactory accuracy across both central and peripheral areas, supporting the reliability of the proposed forecasting framework.\n",
            "\n",
            "Figures 4b d present a comparison between the observed and predicted values for the three analyzed variables: parking meter transactions, revenues, and road occupancy rates. The shaded area labeled as real range represents the interval between the minimum and maximum values observed in the real data at each time step, providing a visual indication of natural variability in the system. The results are segmented by geographical area, with separate plots illustrating central and peripheral areas. Additionally, to assess the specific contribution of individual embeddings used in the models, an ablation study was conducted.\n",
            "\n",
            "This approach allowed us to analyze the relative importance of different variables, highlighting which factors have the greatest impact on the accuracy of the predictions. Full details of this analysis are provided in Section 3E of Supplementary Materials. On one hand, revenue forecasts could support the optimization of parking meter rates, promoting a balanced redistribution of demand across different city areas. On the other hand, occupancy predictions offered key data for designing mobility policies, such as the introduction of reserved or dynamic parking systems, and for planning targeted infrastructure interventions.\n",
            "\n",
            "Finally, the accuracy and robustness of the developed models demonstrate the validity of a data-driven approach to urban mobility management, laying the foundation for further advancements, including integration with real-time data and the implementation of dynamic and adaptive predictive systems. In summary, the results of the predictive analysis highlighted the potential of our framework for smarter and more sustainable mobility. Simulation and what-if scenarios The simulation of scenarios constitutes one of the most advanced and strategic functionalities of the DT framework. This feature enables the assessment of the potential impacts of various decisions and conditions on the urban mobility system within a controlled virtual environment, thereby reducing the risks and costs associated with interventions guided solely by assumptions.\n",
            "\n",
            "By integrating data from diverse sources including predictive models of parking occupancy, economic flows from parking meters, weather conditions, and spatiotemporal variables related to events and POIs the framework facilitates the simulation of complex and realistic scenarios with a high degree of detail. Three key scenarios were simulated to explore urban mobility dynamics: temporary road or area closures, construction of a multistory parking facility, and simulation of consecutive rainy days. The reliability of these scenario simulations depends on the quality of the synthetic data generated within the DT framework. Accordingly, the generative model powering the simulation engine was subjected to a comprehensive evaluation process, including distributional analysis, structural embedding via dimensionality reduction, statistical similarity metrics, and downstream task performance under the Train-on-Synthetic Test-on-Real (TSTR)32,33 paradigm, and a quantitative assessment of simulated interventions across representative what-if scenarios.\n",
            "\n",
            "Full details of these evaluations are provided in Section 3F of the Supplementary Materials, including impact metrics computed on transaction volumes and street-level occupancy under different simulated conditions. Temporary roads or zones closures Temporary closures of roads or urban areas, often necessitated by events, construction projects, or emergencies, present complex challenges for urban mobility management. The framework enabled simulations of these closure scenarios, providing detailed predictions of their effects on traffic patterns and parking occupancy in neighboring zones. 5c, d for an example of parking zone closure simulation.\n",
            "\n",
            "The plots illustrate how the closure of a specific zone triggers an immediate redistribution of traffic to adjacent areas, leading to increased congestion and parking demand Fig. This redistribution often strains areas that were not designed to handle such volumes, creating a cascade effect of congestion and limited parking availability. Radar charts and time series analyses revealed the feedback loop between rising occupancy rates and worsening traffic conditions in surrounding zones. Weekly simulations allowed for a granular evaluation of the cumulative impacts of extended or repeated closures, offering planners the ability to test different mitigation strategies over varying time horizons.\n",
            "\n",
            "By leveraging these insights, urban planners can optimize traffic diversion routes, allocate resources more effectively, and schedule closures to minimize disruptions. This predictive capability enhances the resilience of urban mobility systems, enabling cities to respond proactively to both planned and unplanned disruptions. Construction of a multistory parking facility The development of a multistory parking facility in a strategically selected location was analyzed to evaluate its medium- and long-term impacts on parking distribution and traffic flow. 5e, the addition of parking capacity significantly alleviates congestion in high-demand zones while improving the overall efficiency of the urban mobility system.\n",
            "\n",
            "Simulations demonstrated that the effects of such an intervention extend beyond the immediate vicinity of the facility, as parking demand redistributes to nearby areas. Radar charts highlighted a smoothing of peak occupancy levels across multiple zones, indicating a more balanced utilization of parking resources. Furthermore, when the multistory parking facility was integrated into a high-traffic area, such as a commercial district or major point of interest, it successfully mitigated potential congestion by accommodating larger volumes of vehicles within the facility itself. Strategically locating parking infrastructure near key demand generators enhances accessibility and minimizes disruptions to traffic flow.\n",
            "\n",
            "Impact of consecutive rainy days on zone occupancy To evaluate the influence of weather conditions on parking occupancy patterns, we simulated scenarios in which the city experienced three consecutive days of rainfall. By integrating meteorological data into our generative model, we analyzed shifts in parking demand across different urban zones and examined corresponding changes in user behavior. The results of our simulations highlighted fluctuations in parking demand during prolonged rainy periods, suggesting that weather affects occupancy trends (refer to Fig. However, as these variations did not follow a clearly identifiable trend, we conducted a more in-depth investigation into the spatial distribution of urban areas.\n",
            "\n",
            "Our objective was to determine whether these changes in parking demand were correlated with the presence and density of key POIs, such as commercial hubs, schools, and recreational facilities. This approach allowed us to capture the nuanced relationship between adverse weather conditions and parking dynamics, offering valuable insights into how rainfall influences mobility patterns and decision-making processes among drivers. Framework demonstration The framework platform provides a fully operational and interactive demonstration of our DT system, offering concrete evidence of its capacity to support real-world urban mobility management. Unlike many existing DT studies that remain at the proof-of-concept stage, our DT framework has been deployed and validated on real-world data from the city of Caserta, integrating advanced predictive, generative, and optimization modules into a unified system.\n",
            "\n",
            "A downloadable version of the platform is made available via GitHub34 to ensure reproducibility and facilitate external validation. The web-based interface is structured around the following core components: 1. Interactive urban map: A dynamic geospatial interface allows users to navigate urban zones within the city of Caserta. Real-time environmental data, such as weather conditions and air quality, are retrieved dynamically via API and overlaid onto the map for context-aware decision support.\n",
            "\n",
            "Comprehensive data distribution and statistics: Users can explore key urban mobility metrics through the Data Distribution and Data Statistics tabs. These modules support dynamic visualization (e.g., heatmaps, scatter plots) of spatio-temporal data patterns, enabling detailed inspection of trends and correlations. Agent scheduling optimization: The Agents Calendar tab visualizes weekly shift schedules for enforcement agents, based on our linear programming model. These schedules respect labor constraints and dynamically respond to occupancy trends and violation patterns.\n",
            "\n",
            "Forecasting and predictive modeling: The platform showcases our multivariate spatio-temporal forecasting models for predicting parking meter transactions, revenue, and street occupancy. These forecasts leverage fused historical and real-time data streams and operate at both citywide and zone-specific levels. Generative what-if scenarios: Through the what-if Scenarios tab, users can simulate the impact of strategic interventions, such as temporary road closures, the construction of a multistory parking facility, and extended rainy periods. Each simulation uses our generative conditioned CVAE-WGAN model and is accompanied by detailed visualizations, including radar charts, time series, and occupancy heatmaps, highlighting spatial and temporal effects.\n",
            "\n",
            "The results of this study highlight the efficacy of the proposed framework in addressing specific challenges related to parking resource management within the urban context. By integrating three core components statistical analysis, predictive analytics, and a simulation engine we developed a comprehensive digital DT for the city of Caserta, marking a substantial step forward in urban modeling and decision-support systems. The statistical analysis component provides detailed insights into the city s current state by leveraging diverse data sources to identify patterns and correlations. This founding layer is critical for understanding the complexities of urban dynamics and serves as a platform for more advanced modeling approaches.\n",
            "\n",
            "Our analysis revealed significant and recurring patterns in the usage of parking meters and parking spaces. We examined correlations among various data points, distinguishing between central and peripheral areas of the city. The results indicate that central areas exhibit higher occupancy rates, while both zones experience reduced occupancy during weekends, with a more pronounced decline in peripheral areas. Regular parking occupancy was identified as the dominant trend, although the proportion of irregular use varied across zones.\n",
            "\n",
            "Additionally, a negative correlation was observed between the number of fines issued in a given zone and the rate of illegal parking, suggesting that stricter enforcement is associated with reduced non-compliance. Moreover, parking demand was influenced by external factors such as weather conditions, proximity to POIs, and local events. For instance, on days with heavy rainfall, parking behavior varied significantly depending on proximity to significant POIs, with increased slot occupancy in these areas and decreased occupancy elsewhere. Local events also had a marked impact, increasing transactions in surrounding areas.\n",
            "\n",
            "These findings underscore the importance of incorporating such contextual factors into urban resource planning. To avoid ambiguities in interpreting these relationships, we adopted a multifaceted empirical validation strategy. First, Spearman correlation and Mann-Kendall tests were used to assess statistical associations between external variables (e.g., precipitation, POI density, city events) and parking demand. Then, Granger causality tests were performed to explore temporal precedence, revealing significant directional influence of weather and event variables on parking occupancy dynamics.\n",
            "\n",
            "Finally, we conducted feature ablation analyses within our predictive models, demonstrating substantial performance degradation when contextual features were removed. A key aspect of this study was the automation of agent shift scheduling. The proposed system respects individual working constraints while ensuring equitable distribution of responsibilities across the city. The primary objective is to maximize coverage of paid parking zones, particularly in areas with higher occupancy or illegal parking.\n",
            "\n",
            "Building upon the statistical analysis, predictive models were developed to forecast parking meter usage. These models predict both transaction volumes and revenue generated, incorporating meteorological data, local events, and POIs to estimate parking occupancy rates on individual streets. This capability provides actionable insights into user behavior under varying conditions, enabling more accurate planning and resource allocation. Although the current focus is on intra-urban dynamics derived from high-resolution city-level data, our demand patterns already capture some external pressures, such as regional inflows during peak periods or large-scale events.\n",
            "\n",
            "Moreover, the modular design of our framework allows for future integration with arterial traffic flow data and public transport information, paving the way for intermodal simulation and regional-scale coordination in metropolitan contexts. While our forecasting architecture is based on the established STID model, we emphasize that the contribution of this work lies not in proposing a novel architecture, but in adapting and benchmarking a robust model within a fully integrated digital twin framework that combines multivariate forecasting, generative simulation, and operational decision support for real-world urban mobility. The predictive analytics component also served as the foundation for generative models capable of simulating various scenarios and outcomes based on different inputs. By adopting a simulation-based approach, we integrated multiple spatial and temporal variables, offering a holistic understanding of decision impacts on urban mobility.\n",
            "\n",
            "This approach not only optimizes current resources but also supports long-term strategic planning for sustainable and resilient urban management. To maintain robustness over time and ensure adaptability to evolving urban dynamics, the system includes a periodic model retraining strategy. This approach allows the predictive and generative components to incorporate the most recent data, adapting to shifts such as new mobility patterns, regulatory changes, or unexpected disruptions. As a result, the overall performance of the system improves continuously, preserving its reliability in the face of long-term and abrupt transformations.\n",
            "\n",
            "Another key aspect of DT systems, particularly in critical infrastructure domains, is their ability to reflect and respond to real-world dynamics in near real-time. In this work, we adopted a practical yet flexible approach to temporal resolution: the DT operates at an hourly granularity, synchronizing heterogeneous data sources including parking sensors, parking meters, environmental conditions, and urban events. This temporal resolution allows for sufficient detail to capture urban dynamics while maintaining computational efficiency and system responsiveness. Internally, the system performs daily inference by downloading the latest available data via API to generate updated forecasts and scenario simulations.\n",
            "\n",
            "While inference is fast, model retraining is performed monthly to incorporate evolving trends such as peak-hour shifts, seasonal dynamics, or hypothetical changes in the urban environment (e.g., new municipal policies, road modifications, or infrastructure updates). By the way, this is a parameter that can be changed if necessary. While the predictive and generative models are not continuously retrained, the system is designed to operate in streaming mode, allowing for continuous integration of updated data streams. This makes the digital representation capable of supporting decisions on a near real-time basis, such as anticipating congestion hotspots, reallocating enforcement agents, or evaluating short-term interventions.\n",
            "\n",
            "Nonetheless, we acknowledge that a fully closed feedback loop in which decisions or actions derived from the DT directly modify the physical system and those effects are immediately reintegrated into the DT is not yet realized. At present, the DT framework is structured as a decision-support tool, where insights and forecasts guide human-in-the-loop interventions (e.g., scheduling changes, policy adjustments). This is consistent with many municipal use cases, where interventions are not automated but are instead mediated through urban mobility managers. However, our framework is modular and can be extended to interface with control systems (e.g., for dynamic signage or pricing), laying the groundwork for a semi-automated feedback loop.\n",
            "\n",
            "We also recognize the importance of maintaining alignment between the physical and digital systems over time. Discrepancies, particularly during periods of high variability such as peak hours or special events, may lead to drift between the DT s predictions and actual system behavior. To mitigate this, we integrate periodic retraining procedures, as well as monitoring of residual prediction errors, to ensure the DT remains robust and accurate over time. While parking systems may not fall under critical infrastructure per se, reliability and safety are central to the broader applicability of DT systems.\n",
            "\n",
            "In future extensions, we envision adding forecast confidence bands, adaptive alert mechanisms, and fail-safe controls that limit or filter decision-making recommendations under high uncertainty. These strategies will increase the robustness and reliability of the DT as it moves closer to operating as a true cyber-physical system. In summary, although a direct and fully automated feedback loop is not currently active, our system already implements a structured feedback mechanism through its monthly retraining strategy. By regularly incorporating new data collected after prior interventions, via API integration with the infrastructure management system, the DT adapts its models to reflect changes in user behavior, system performance, and policy outcomes.\n",
            "\n",
            "This ongoing update process ensures that the DT evolves in response to the very interventions it informs, closing the loop in a practical and operational way. Moreover, our CVAE-WGAN framework offers strong potential for deepening this integration. Its generative and conditional architecture can be extended to simulate sequences of decisions and their cumulative effects, allowing future versions of the DT to model recursive system dynamics and long-term strategy impacts. This positions the framework for progressive evolution toward more autonomous and adaptive urban infrastructure management.\n",
            "\n",
            "The scalability of the framework is central to its applicability in broader urban contexts. While this study focuses on the city of Caserta, our architectural design prioritizes modularity, computational efficiency, and horizontal extensibility. The framework supports scalable forecasting and simulation workflows through entity-level modeling and supports parallel retraining or distributed inference as data volumes grow. To evaluate current performance, we conducted a series of experiments analyzing training time as a function of input size, time window length, and number of modeled entities.\n",
            "\n",
            "S6 of the Supplementary Materials, show approximately linear growth in training time per epoch and stable inference times across all configurations tested confirming the tractability of the system at city scale. The core components are amenable to parallelization, and scaling up to larger metropolitan environments can be achieved without altering the framework s structure. These findings highlight both the practical strength of the current implementation and its readiness for future expansion. We emphasize that the goal of this work is not to propose a city-specific solution, but to develop a reusable architecture grounded in real-world deployment.\n",
            "\n",
            "While testing the system in another large-scale metropolitan context would constitute a separate study, our results show that the framework performs well at its current scale and is architecturally ready for horizontal expansion. Additionally, many core design decisions such as entity-based modeling, zone-aware shift scheduling, and exogenous feature integration generalize naturally across cities, enabling straightforward transfer with localized retraining and calibration. A natural extension of the proposed framework lies in its integration with other digital twin subsystems within a smart city ecosystem. While our current implementation focuses specifically on parking management and local mobility forecasting, urban mobility is inherently intertwined with broader infrastructural domains such as public transportation, traffic flow, energy usage, and shared mobility.\n",
            "\n",
            "For instance, an increase in bus frequency or the introduction of new micro-mobility services could directly impact parking demand, influencing both the predictive and generative components of the system. Our architecture is explicitly designed to accommodate such interactions: exogenous factors such as transit availability, real-time road network telemetry, or energy constraints can be incorporated into the forecasting and simulation pipelines as contextual embeddings or external conditioning variables. This modular design facilitates incremental integration with adjacent digital twins and supports scenario evaluation across interdependent urban subsystems. Furthermore, the framework includes generalized interfaces for data ingestion, synchronization, and model orchestration, enabling future expansion beyond parking.\n",
            "\n",
            "By abstracting core functionalities (e.g., data preprocessing, inference, and resource scheduling), the system supports extensibility to other domains while maintaining consistency in operational logic and temporal resolution. This architecture thus lays the foundation for interoperable, multi-domain digital twins capable of supporting coordinated decision-making in complex urban environments. The digital twin developed in this study serves as a decision-support tool tailored for parking management and local urban mobility planning. By enabling the simulation and evaluation of parking-related scenarios before implementation, the framework supports operational planning and evidence-based decision-making.\n",
            "\n",
            "While its current focus is limited to parking systems, the proposed approach demonstrates the potential for enhancing resource allocation and responsiveness to localized urban challenges. Although rigorous preprocessing ensured a consistent dataset, the absence of granular environmental metrics and detailed user behavior patterns constrained the scope of the analysis. Expanding data sources and incorporating real-time streams would enhance the granularity and depth of insights. In addition, ensuring robustness against partial sensor failures is a key consideration in real-world deployment.\n",
            "\n",
            "To mitigate this, the system includes signal filtering and imputation techniques that allow the model to maintain performance even in the presence of intermittent data loss or faulty sensors. Future versions could incorporate more advanced anomaly detection and fault-tolerant design, particularly for long-term sensor degradation or large-scale outages. Ablation studies on the embeddings constituting the predictive models were utilized to improve explainability, but further efforts are needed to make results accessible to non-technical stakeholders. Enhanced transparency will bolster stakeholder confidence and facilitate broader adoption of the system.\n",
            "\n",
            "The framework can be further expanded through several promising avenues. Integrating advanced simulation methods such as agent-based modeling and system dynamics could offer deeper insights into the complex interactions among socio-economic, environmental, and infrastructural factors. In terms of shift scheduling, upcoming improvements will focus on integrating agents personal preferences, including preferred shift types (morning, afternoon, evening), weekend availability, and flexibility to accommodate leave or sick-day requests. These enhancements aim to support a more personalized and adaptive work environment.\n",
            "\n",
            "In scenarios where full coverage of all zones is not feasible, due to limitations in available personnel or time slots, the scheduling algorithm strategically reallocates agents to higher-priority areas, based on factors such as historical occupancy, enforcement effectiveness, and compliance levels. This adaptive prioritization ensures that limited resources are directed toward zones with the highest operational impact. As part of our ongoing efforts to build a comprehensive understanding of urban mobility, we have also started collecting data on traffic flows within the Limited Traffic Zone (ZTL), covering both authorized and unauthorized vehicles. This information provides a more complete picture of mobility patterns and underscores the potential of ZTL policies as tools for regulating parking behavior, reducing traffic congestion, and mitigating air pollution.\n",
            "\n",
            "The emphasis on traffic data is motivated by two factors: first, exploratory analysis revealed strong temporal and spatial correlations between traffic volumes and parking availability in both central and peripheral zones; second, traffic flow data is operationally valuable for urban mobility planners, as it directly informs enforcement strategies, pricing schemes, and resource allocation. While other influencing factors such as socio-economic segmentation or behavioral profiles are also relevant, traffic remains one of the most actionable and interpretable levers for policy design and real-time control. By exploring new what-if scenarios, this research investigates how potential adjustments to ZTL regulations could impact parking demand, traffic behavior, and overall mobility in the urban context. Finally, the DT system s capacity to incorporate real-time data from parking occupancy sensors to traffic flow analytics will further enhance urban mobility management.\n",
            "\n",
            "These advancements will enable dynamic resource allocation, more accurate scheduling, and increased responsiveness to evolving urban conditions. This section presents the key points, describes the data used, analyzes the various available sources, and illustrates the data fusion process. A detailed description of the methods and models used to develop the DT framework is then provided. Finally, the experimental setup for the presented case studies is described.\n",
            "\n",
            "Managing urban mobility is a complex challenge, especially in medium-sized or large cities, where balancing the needs of citizens, available infrastructure, and environmental impacts is crucial. The primary issue lies in the need to optimize resource allocation, improve service efficiency, and mitigate the negative effects of external factors such as traffic and pollution, while maintaining a system that is flexible and adaptable to the ever-changing dynamics of urban life. To systematically formulate the problem, we consider the city as a dynamic system characterized by: Entities - Key elements of the system, such as vehicles, pedestrians, agents, and infrastructure (e.g., parking meters, slots, sensors). Relationships - Interconnections between entities, including city events, parking area usage, and interactions with the surrounding environment.\n",
            "\n",
            "Constraints - Rules and limitations, such as restricted traffic zones, access schedules, infrastructure capacity, and environmental regulations. Objectives - Minimizing congestion and pollution, maximizing the utilization of available resources, and improving user experience. Our formulation combines descriptive, predictive, and generative models that capture current dynamics and simulate future scenarios. This integrated approach enables a holistic solution to the problem, accounting for the complex interactions between mobility, infrastructure, and the environment.\n",
            "\n",
            "Data sources overview The construction of the DT framework for the city of Caserta is grounded in the integration of diverse and multi-scale data sources. These include operational, contextual, and environmental data streams, which together enable a rich, real-time representation of urban mobility dynamics. The main data categories are: Infrastructure and sensor data: This includes IoT signals from parking meters and parking slot occupancy sensors (see Fig. 7a, b), which provide real-time information on transaction activity and space utilization.\n",
            "\n",
            "Parking meters are linked to detailed transaction logs, while slot sensors capture changes in occupancy status with high temporal resolution. Parking enforcement data: Handheld GPS devices used by municipal agents (Fig. 7c) track their patrol activities, capturing detailed shift data, street-level trajectories, and vehicle checks. Associated records include scanned license plates, validity assessments (e.g., permits, subscriptions), and issued fines.\n",
            "\n",
            "These datasets support simulations of agent deployment and control policy effectiveness (see Fig. Administrative and regulatory datasets: Additional databases provide structured information on the issuance of pre-notifications, fines, permits, and subscriptions. This data enables a fine-grained understanding of compliance patterns and the impact of regulatory mechanisms on user behavior. Contextual data: Points of Interest were extracted and categorized using the OpenStreetMap API, covering healthcare, cultural, educational, commercial, and service-oriented amenities.\n",
            "\n",
            "A curated city event calendar includes cultural, musical, and civic events with geolocation and time stamps, capturing demand shocks in parking behavior. Environmental and meteorological data: Hourly weather conditions (e.g., temperature, precipitation, wind speed, humidity) were collected via the Open-Meteo API and integrated to model their effect on mobility decisions. Additionally, air quality indicators (PM2.5, PM10, NO2, O3) were incorporated based on the European Air Quality Index (AQI), enabling the exploration of environmental impact on urban dynamics. These heterogeneous sources are synchronized through a unified data preprocessing and fusion pipeline, allowing the DT to generate accurate forecasts and run realistic what-if simulations.\n",
            "\n",
            "The data structure supports fine spatial resolution (per-street or per-sensor) and a temporal granularity ranging from hourly to weekly intervals. A detailed technical description of each data source, including formats, schemas, aggregation strategies, and temporal characteristics, is available in the Supplementary Materials. Data preprocessing Ensuring the accuracy and consistency of data is a fundamental requirement for the effectiveness of the DT framework. Data cleaning and integration are essential steps to transform raw data into a reliable resource, ready for analysis and simulation.\n",
            "\n",
            "These processes address issues such as noise, missing values, inconsistencies, and duplicates, while unifying heterogeneous datasets into a cohesive and homogeneous structure. The initial phase focused on data format standardization, tackling the structural diversity of the collected datasets, which spanned formats such as CSV, JSON, and XML. Moreover, these sources differed significantly in their temporal and spatial resolution: for example, parking sensors provide minute-by-minute updates, whereas weather data is reported hourly. To reconcile these temporal discrepancies, we developed synchronization routines based on interpolation and temporal aggregation, effectively aligning all data streams within a unified temporal framework.\n",
            "\n",
            "Subsequently, the data cleaning phase aimed to enhance the quality of the collected information. This goal is achieved through noise reduction, handling of missing values, and standardization of formats. Noise reduction involves the elimination of irrelevant or erroneous information that could distort analyses. Advanced filtering techniques, including statistical analyses of distributions and contextual validation rules, were employed to detect and remove outliers and irrelevant data.\n",
            "\n",
            "To better analyze and understand the data, we applied the STL decomposition35 to the data related to parking meter transactions and parking slots occupancy, which allowed us to separate the time-series into seasonal, trend, and residual components. This approach facilitates a clearer understanding of underlying patterns and enhances the overall data analysis process. A detailed explanation of this decomposition and its role in the analysis is provided in Section 2A of Supplementary Materials. POIs embedding In this section, we describe the process of generating a latent representation for POIs, which are integrated into models to improve accuracy.\n",
            "\n",
            "The approach for representing POIs consists of two main components: the encoding of the POI category and the encoding of the distances between the POIs and the considered entities, in this case, the parking meters. Each POI is characterized by its geographical coordinates (latitude and longitude), along with additional information such as its name and category. For each parking meter i, we define a set of associated POIs as: P _ ( t _ , d _ ) j 1,, N _ (1) where tij is the category of POI j associated with parking meter i. We only consider POIs that belong to the categories described in the previous section; dij is the geodetic distance between parking meter i and POI j; and Ni is the total number of POIs associated with parking meter i.\n",
            "\n",
            "This choice was guided by considerations of computational efficiency and model generalizability. To assess the impact of alternative metrics, we performed a comparative analysis using pedestrian walking distances computed over the street network. The results, presented in Section 2D and Table S3 of the Supplementary Materials, support the use of geodetic distance as a valid and efficient for POI proximity in the context of spatial embeddings aimed at knowledge representation rather than physical routing. Then each pair (tij, dij) is transformed into a dense numerical embedding defined as: e _ _ ( t _ , d _ ), (2) which combines both the category encoding and the distance encoding.\n",
            "\n",
            "The POI categories are transformed through an embedding layer into dense numerical vectors, allowing the model to learn and represent the relationships between different categories in a continuous space. The distance embedding, on the other hand, is based on the normalized distance between a POI and the reference parking meter. This normalized distance is then converted into a numerical vector via a linear function that combines the distance with a set of learnable weights. Finally, the category and distance embeddings are concatenated to form a unified combined vector.\n",
            "\n",
            "This representation is subsequently processed by a fully connected network to generate the complete embedding eij. Finally, for each parking meter i, the embeddings of all associated POIs are aggregated into a single vector representing the overall interaction with the POIs: e _ ( e _ ,j 1,, N _ ) (3) where the Aggregator function can be implemented as a weighted average, a sum, or a pooling operation (e.g., max pooling). In our implementation, a weighted average with respect to Ni is used. Streets occupancy rate To analyze the occupancy of parking spaces within different streets, the occupancy rate was calculated for each street during each time interval.\n",
            "\n",
            "In our implementation, the time interval is set to an hourly granularity, which allows us to balance temporal resolution with signal stability. The calculation process is based on aggregating occupancy data from sensors installed in the parking slots. These sensors continuously monitor the availability of the slots, providing real-time information about their occupancy status. The raw data was preprocessed and filtered to ensure that only relevant signals are included, such as those with status_change 1 and occupied 1, which indicate a change in the occupancy state of the slots.\n",
            "\n",
            "The next step involves grouping the information for each parking stall and each time interval to compute the number of occupied stalls during each hour. The occupancy rate Ri,j for each street i in time interval j is calculated as the ratio of occupied stalls to the total number of available stalls, expressed as: R _ _ k 1 N _ 1 _ ( d _ , , ) N _ (4) where Ni is the total number of stalls on street i, and 1occupied(di,j,k) is the indicator function that takes the value 1 if stall k on street i is occupied during time interval j, and 0 otherwise. Framework architecture Data statistics This section outlines the methodologies underpinning the first component of our DT, which focuses on organizing and analyzing system data to capture their interconnections. This process is essential for supporting the development of predictive models and optimized strategies for urban mobility management.\n",
            "\n",
            "The initial step involves associating parking slots with the devices (device IDs) installed in them. Since sensors can be relocated over time, it is crucial to accurately map devices to their corresponding parking slots using historical data. Once this phase was completed, informational events were isolated to eliminate duplicates, thereby enhancing analytical precision. It was observed that sensors could transmit signals even when a vehicle merely passed through their detection range without parking in the slot.\n",
            "\n",
            "To filter out such events, a signal was considered valid only if it recorded a state change (from occupied to free or vice versa) with a minimum interval of 60 s. This threshold was determined through empirical analysis and validation testing. Additionally, for parking meters, anomalous sensors with erroneous coordinates likely due to recording errors were removed. The initial statistical analyses focused on the distribution of parking meter transactions, street occupancy rates, and STL decomposition to identify recurring patterns, seasonality, anomalies, and trends in the data.\n",
            "\n",
            "Furthermore, the Granger causality test36 was performed to assess whether one time series could predict another, providing insights into causal relationships between variables. To ensure the reliability of these time series analyses, the Augmented Dickey Fuller (ADF) test37 was applied to verify the stationarity of the series. Stationary series are critical for robust predictive modeling, as they allow the use of standard forecasting techniques without additional transformations. A critical aspect of the analysis is the integration of external factors, such as weather conditions, air quality, POIs, city events, and public holidays, as these significantly influence traffic dynamics and parking usage.\n",
            "\n",
            "Our analysis began with weather and environmental data, which share a similar structure as they are sourced from the same database via an API. Following the selection of relevant features for developing the DT framework, a preprocessing phase was initiated. This phase included hourly sampling for the entire available period and data normalization. Subsequently, correlations between these external factors and sensor data were examined.\n",
            "\n",
            "Spearman s correlation38 was employed, as it is particularly suited to analyzing relationships between variables that do not necessarily follow a linear distribution. This correlation analysis highlighted the need to focus on time series behaviors during specific weather events. This phase involved identifying and analyzing periods when adverse weather conditions, such as heavy rainfall or extreme temperatures, impacted urban mobility. Observing these events enabled the detection of changes in traffic patterns and parking usage, providing critical insights for designing more robust predictive models.\n",
            "\n",
            "Moreover, the analysis of city events, POIs, and public holidays aimed to understand how temporal patterns vary during special occasions or how proximity to POIs or event locations influences these patterns. The analyses encompassed not only the number of transactions or slot occupancy but also the type of occupancy, distinguishing between legal and illegal parking. Specifically, the relationship between overall occupancy rates and illegal parking rates was studied. Statistical tests, such as the Mann-Kendall test39,40 and correlation analysis, were applied to examine trends in unauthorized parking and fines issued across different areas and times, with the goal of identifying priority intervention zones.\n",
            "\n",
            "Sen s Slope Estimator41 is employed to quantify the magnitude of trends identified by the Mann-Kendall test. For this analysis, we focused on the paid parking time specific to each zone. The streets of the city of Caserta are categorized into two zone types: central and peripheral areas. The central area enforces paid parking hours from 8:00 to 21:00 on weekdays and from 8:00 to 24:00 on weekends.\n",
            "\n",
            "In contrast, peripheral area enforces paid parking hours from 8:00 to 14:00 and from 16:00 to 21:00 on weekdays, and from 8:00 to 14:00 and from 16:00 to 24:00 on weekends. Consequently, we divide the day into three time slots: morning (8:00 to 14:00), afternoon (14:00 to 16:00), and evening (16:00 to 24:00). For each time slot, we applied the Mann-Kendall test to the hourly number of fines issued and to the zone-specific abusivism index, defined as the total number of minutes of unauthorized parking divided by the number of slots available in the zone. Regarding enforcement data, a rigorous preprocessing methodology was applied to the data collected by agents.\n",
            "\n",
            "This includes information on work schedules, locations, assigned zones, and actions performed. GPS data were filtered to include only those points corresponding to agents shifts and positions within designated areas, excluding data with unrealistic speeds or recording errors. Activities performed by agents, such as vehicle scans or fine issuance, were analyzed to assess the distribution and effectiveness of interventions. Analyzing these data is critical for evaluating the current setup, implementing automated scheduling, and assessing agents performance.\n",
            "\n",
            "Predictive analytics Key performance indicators In the context of operational monitoring and performance evaluation, KPIs are crucial for assessing the efficiency, reliability, and overall productivity of agents. These metrics enable the objective measurement of an agent s activities, encompassing adherence to schedules, time management, zone coverage, and task execution. Analytical frameworks further integrate contextual variables, such as weather conditions and compliance with assigned zones, ensuring a comprehensive and equitable assessment. It provides operational insights by systematically categorizing performance data, identifying areas of high efficiency, and uncovering deficiencies in zone monitoring or task execution.\n",
            "\n",
            "It supports informed decision-making by offering actionable insights to supervisors for performance reviews and strategic planning. It ensures fair evaluation by accounting for external factor, such as weather conditions, which influence workload and effort. Finally, it promotes productivity enhancement by incentivizing agents to improve schedule adherence, optimize zone productivity, and maintain regulatory compliance. Computational details on the calculation of KPIs are available in Section 2B of the Supplementary Materials.\n",
            "\n",
            "To provide a broader and more balanced assessment of an agent s performance, we calculated monthly statistics that aggregate key performance metrics across all shifts worked in a given month. These statistics are crucial for understanding trends, identifying outliers, and ensuring a fair evaluation of agents over a longer time frame. By aggregating performance data, we can account for variations that may occur in individual shifts due to specific circumstances, such as weather anomalies or operational disruptions, and provide a more robust representation of each agent s contributions. The monthly statistics include the total number of shifts assigned to each agent and the number of those shifts that were worked, canceled, or deemed unreliable due to anomalies in the data.\n",
            "\n",
            "The total hours worked by each agent were also calculated, along with the cumulative delays in entry times and early exits, expressed in hours and minutes. These metrics provide insight into an agent s reliability and adherence to schedule. We also calculated the average walking times, distinguishing between walking performed within the designated zone, outside the zone, and overall. Additionally, average values for zone coverage, productivity, and abusiveness are computed, offering a comprehensive perspective on how effectively agents manage their responsibilities across the areas they monitor.\n",
            "\n",
            "Each of these metrics is normalized to a scale between 0 and 1, ensuring consistent comparability. Further, the monthly statistics include average times dedicated to specific tasks, such as checks, warnings, general and disabled permit management, and ticketing. These averages help evaluate how agents allocate their efforts to different activities, shedding light on their overall productivity and task prioritization. A key component of the monthly evaluation is the performance score, which serves as a comprehensive indicator of an agent s efficiency and effectiveness.\n",
            "\n",
            "This score is calculated using a formula that combines the sum of the normalized shift scores with the product of the agent s average shift score and the total hours worked during the month. The performance score is designed to objectively reflect an agent s work contribution, ensuring that agents who work fewer shifts or hours receive a proportionally lower score, even if their individual shift scores are high. This method ensures that agents who have not consistently ensured their presence, due to canceled shifts, excessive delays, or early exits, are not excessively rewarded even if they achieved high scores, as their lack of reliability represents a challenge for the organization. Additionally, to assess performance at a more granular level, we computed the performance score by zone.\n",
            "\n",
            "This metric evaluates an agent s performance in each specific zone they monitored during the month, applying the same formula used for the total performance score. For shifts covering multiple zones, both the shift score and hours worked are distributed proportionally across the zones. This ensures a fair and accurate representation of an agent s contributions to each zone, allowing for a detailed comparison of performance across different areas of responsibility. Linear model for generating the agents shift calendar Our DT system is designed to generate weekly shift schedules for agents, aiming to optimize zone coverage, ensure fair rotation of personnel across different areas, and manage work shifts efficiently.\n",
            "\n",
            "Additionally, the system allocates resources with the goal of maximizing revenue while balancing operational demands and organizational constraints. Another key objective is to enhance the performance of individual agents, ensuring that their capabilities are utilized to their full potential within the framework of the shift scheduling process. The shift scheduling algorithm is formulated as a Linear Programming (LP) problem, a mathematical optimization technique extensively employed in operational research and resource allocation. LP enables the optimization of an objective function subject to a set of linear constraints.\n",
            "\n",
            "Its broad applicability spans various domains, including logistics and workforce management, due to its efficiency and adaptability in solving complex problems42. By modeling the scheduling challenge as an LP problem, the algorithm ensures a systematic and objective to resource allocation, balancing operational demands with organizational constraints. The linear programming formulation can be found in Section 2C of the Supplementary Materials. For each shift, morning, afternoon, or evening, the scheduling algorithm generates a schedule that assigns agents and specifies the duration of their shifts.\n",
            "\n",
            "Starting from this solution, the algorithm further distributes the time slots assigned to each agent. Specifically: Morning shift - Agents assigned to this shift are allocated time slots based on the presence or absence of coverage in subsequent shifts. If the zone is covered by another agent in the afternoon, the time slot is set to avoid overlap and ensure smooth transition. Otherwise, the time slot is extended to cover a broader period of the morning and early afternoon.\n",
            "\n",
            "Afternoon shift - Time slots for this shift are determined based on the coverage provided during the morning. If the zone was covered earlier, the time slot is allocated to minimize overlap with the previous shift. If no coverage exists, the time slot starts early enough to ensure sufficient coverage for the zone. Evening shift - The time slots for the evening are assigned to minimize overlap with the afternoon shift, if coverage exists.\n",
            "\n",
            "If the zone is uncovered during the afternoon, the time slot is configured to cover the final hours of the evening. In cases involving multiple agents, time slots are distributed to minimize overlaps while ensuring comprehensive coverage. In all cases, if an agent is responsible for multiple zones, the same time slot is assigned to all zones managed by that agent. Similarly, if multiple agents are assigned to the same zone within a shift, the time slots are divided to minimize overlaps and ensure full coverage.\n",
            "\n",
            "Deep Learning based model for prediction The predictive phase of our DT framework was realized using the Spatial-Temporal IDentity (STID) model, a simple yet effective baseline for the prediction of multivariate time series. STID, a work by Shao et al.43, is designed to address the problem of indistinguishability of spatial and temporal information in multivariate data. The STID model was adapted for our use case, considering the specific characteristics of parking meter data in the city of Caserta and the integration of exogenous variables and POIs. Specifically: The spatial embedding is designed to represent the relationships between different entities (e.g., parking meters, slots) based on their geographical locations.\n",
            "\n",
            "This is achieved using a combination of techniques based on the geographic distance matrix and the Multidimensional Scaling algorithm44 (MDS). This spatial representation is crucial for capturing the influence of geographic distances on the data, improving the model s ability to make accurate predictions in urban contexts. The encoder for exogenous features is designed to integrate external information effectively into the model, ensuring coherent representation and improved predictive capacity. The embedding for POIs is designed to capture the influence of nearby structures and locations on the analyzed entities.\n",
            "\n",
            "The architecture includes an embedding layer that maps each POI type into a latent space of dimension . By integrating POIs and exogenous variables, the adapted STID model demonstrated excellent predictive performance, capturing both internal dynamics (temporal and spatial patterns of parking meters and parking slots) and external influences. The predictions for transactions, revenues, and occupancy outperformed baseline models, as shown in Supplementary Data 4 to 6, validating the model s capability to analyze complex scenarios and generate reliable simulations for decision support. Full details of the model architecture, implementation, and training procedure are available in Section 2D of the Supplementary Materials.\n",
            "\n",
            "To validate the modeling choice, we also performed a comparative evaluation against univariate forecasting models. As summarized in Table 1, the univariate version of the proposed model clearly outperforms classical baselines such as SARIMAX and LSTM across multiple data types. The table also includes the multivariate version of the model, which represents the final chosen configuration. A more detailed breakdown of the results can be found in Table S7 of the Supplementary Materials and in Supplementary Data 1.\n",
            "\n",
            "In particular, Table S7 reports full metric values for all univariate models, while Supplementary Data 1 shows that the multivariate version consistently outperforms its univariate counterpart, underscoring the added value of capturing spatial dependencies. To further evaluate the robustness of the model over extended forecast horizons, we analyzed the evolution of predictive uncertainty in long-term scenario forecasts. As detailed in the Supplementary Materials (Section S3B), we evaluated the accumulation of forecast errors over a 56-day prediction window across all target variables. S4 S5, reveal a nonlinear pattern of error growth, marked by a steep rise during the initial 7 10 days, followed by a progressive stabilization in subsequent periods.\n",
            "\n",
            "Throughout the forecast window, predicted trends remain aligned with observed dynamics, and the associated uncertainty bands continue to provide meaningful information. These findings demonstrate the model s ability to capture essential temporal structures and reinforce the need for ongoing model refinement to preserve predictive accuracy over time. Simulation engine Construction of input data The creation of the input matrix Y is a central element of the pipeline designed for simulating historical, real-time, and what-if scenarios in urban mobility. Its design aims to accurately capture the temporal and spatial dynamics of the analyzed urban system, ensuring a consistent and comprehensive representation of the information required for training and validating the model.\n",
            "\n",
            "The matrix is defined as: N M T (5) where N represents the number of entities (e.g., sensors or parking spaces), M represents the number of input states associated with each entity, and T denotes the number of time steps. Each element yi,j,t in the matrix represents the value of state j for entity i at time t. For each entity i, the process begins with an initial dataset Di containing raw values recorded over various time steps. A temporal aggregation function fagg is applied to summarize the observations within each time window Δt: y _ , , f _ ( y _ , , _ : t _ t,t t ), (6) where tk represents the time steps within the window Δt 4.\n",
            "\n",
            "In our implementation, the decision to use a 4-h aggregation interval was made in alignment with the operational needs of the stakeholder; simulation scenarios were designed to match their internal planning and monitoring processes, which typically operate at this resolution. Moreover, 4-h intervals align with the structure of paid parking time windows, which is the focus of both enforcement and infrastructure planning activities. This granularity thus supports stakeholder-relevant analysis while ensuring data stability and tractability. Building on this setting, each feature yi,j,t is normalized, and for each time window Δt, an intermediate matrix Yt is constructed: _ .\n",
            "\n",
            "(7) The matrices Yt generated for each time window are concatenated along the temporal axis to form the final matrix Y: _ 1 , _ 2 ,, _ . (8) Each entity was georeferenced using its (lat, lon) coordinates. These spatial details were incorporated into the matrix to embed a spatial representation, enabling the model to exploit spatial relationships among parking spaces during predictive analysis or scenario generation. The resulting matrix Y, enriched with spatial metadata, offers an optimal combination of temporal and spatial information.\n",
            "\n",
            "This structure is fundamental for understanding occupancy patterns and supporting the design of intelligent systems for urban mobility management. For the simulation of what-if scenarios, a slightly different approach have been approached, particularly in terms of spatial saving. Instead of the regular dataset construction process, we focused on accurately capturing and saving the spatial relationships of entities, which is key to simulating various scenarios in urban mobility. 9, the first step was to establish a bijective mapping between each entity s geographic coordinates (latitude, longitude) and its corresponding grid indices (i, j) within a predefined spatial grid of size H and W.\n",
            "\n",
            "This mapping ensures that each entity s spatial position is consistently represented on the grid, facilitating spatial analysis. After establishing the grid layout, we applied temporal aggregator over a time window Δt, as done in the prediction dataset construction. Specifically, for each time window, we used the aggregation function fagg to summarize the data for each feature. This function operates over the time steps within the window, and the aggregated values are assigned to the corresponding grid positions, providing a spatially structured representation of the data.\n",
            "\n",
            "The resulting matrices for each time window are finally combined into a final tensor for the entire simulation period. CVAE-WGAN model architecture The generative phase of our DT framework is implemented using a customized version of the Conditional Variational Autoencoder-Wasserstein Generative Adversarial Network (CVAE-WGAN), specifically designed to address the challenges of multivariate time series generation. This framework, inspired by the principles of Kingma et al.45 and Arjovsky et al.46, leverages the strengths of variational encoding for latent space representation and adversarial training to ensure high-quality and realistic outputs. This approach integrates 3D convolutions46 and LSTM-based convolutions47 to effectively extract complex spatial-temporal patterns from multivariate inputs and enhance the model s ability to learn long-term temporal dependencies.\n",
            "\n",
            "By conditioning the generative process on external inputs, such as domain-specific spatial features masks, this approach is particularly suited for what-if scenario analysis. It enables the exploration of hypothetical scenarios, enabling stakeholders to evaluate the potential impact of changes in key variables through a fully data-driven approach. Following the methodology outlined in the previous section, we used the time series data from both parking meters and parking slots to construct two distinct spatio-temporal matrices. Each matrix represents the temporal evolution of the respective variable across the spatial grid, effectively capturing both spatial and temporal dependencies within the dataset.\n",
            "\n",
            "Subsequently, these matrices were concatenated to form a unified input tensor, which was processed through the customized CVAE-WGAN model, and transforms the spatio-temporal data into meaningful latent representations for generative tasks. To achieve this, the model incorporates a series of specialized components, including encoder for latent space mapping, a generator conditioned on external inputs, and a critic to refine the quality of the output. Specifically: Encoder - The encoder processes the input tensor representing a set of conditions derived from the data tensor, through the application of a domain-specific mask, which captures features such as external influences in spatio-temporal format. This input is viewed as a sequence of spatial maps over time, similar to a video sequence, where each map (frame) corresponds to a temporal snapshot of the mask48.\n",
            "\n",
            "To process this input, the encoder is structured into four convolutional blocks, succeeded by three ConvLSTM layers, allowing the model to learn a smooth and continuous latent space representation and to provide a rich contextual information to better guide the generation of what-if scenarios. Generator - The generator starts from the concatenation of a noise vector and the latent vector obtained by the encoder. This combination is then processed trough two Linear layers, which transform and expand them into a suitable tensor for the ConvLSTM layers, matching the intermediate LFcond output by the encoder. To further refine the temporal and spatial features, a Cross-Attention Mechanism49 is applied.\n",
            "\n",
            "In the end, the generator transitions to a deconvolutional phase, which mirrors the structure of the encoders. Through this combination of latent space fusion, temporal modeling, attention-guided refinement, and deconvolution, the generator produces high-fidelity spatio-temporal outputs that align with both the data and the conditions provided as inputs, obtaining the reconstructed tensor. Critic - The critic processes the input tensor X and the generated tensor , both of the same dimensions. It is constructed using the same number of convolutional blocks as the encoders, but the convolutional operations are applied across all three dimensions: spatial H, W and temporal T.\n",
            "\n",
            "A diagram of the presented architecture is presented in Fig. The entire training process was structured to optimize the model s performance across all its components. The training procedure alternated between updating the critic and the generator (as well as the VAE components) to achieve both adversarial training and variational inference. The key losses during training include: L _ (X,) C T H W _ c 1 C _ t 1 T _ i 1 H _ j 1 H x _ , , , - _ , , , _ (9) where DKL represents the Kullback-Leibler Divergence, p(z) is the prior distribution, and q(zcond X) is the learned posterior distribution for the latent variables from the encoder45.\n",
            "\n",
            "Additionally, the critic loss LC and generator loss LG are calculated as: L _ (X) () , L _ - () (10) and optimized using the Adam algorithm. Data splitting followed a temporal configuration (80 training, 20 validation), using a window size of T 42. In our setting, we set T 42, corresponding to a 1-week horizon divided into 4-h parking intervals. This configuration was motivated both by our statistical analysis, which revealed strong weekly seasonality in transaction counts, amounts, and occupancy time series, and by the operational requirements of our stakeholders, who are interested in scenario simulations aligned with the time windows of enforced parking policies.\n",
            "\n",
            "It also complements the 4-h aggregation step (Δt) described earlier. For the construction of the starting tensor, we created maps with dimensions 100 100. This approach allows for robust scenario simulations, enabling stakeholders to explore potential impacts of changes in key variables. Full technical details, including the architecture and training methodology, are available in Section 2E of the Supplementary Materials.\n",
            "\n",
            "A comprehensive illustration of the internal workflow, including sequential and feedback interactions between components (e.g., forecasting-guided scheduling, synthetic-data-based evaluation, and stress-tested planning), is provided in the Supplementary Materials (Section 2F, Framework Workflow, Figs. This visual breakdown highlights how modules interact, share data, and support decision-making under both routine and hypothetical conditions.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Compare raw vs cleaned content\n",
        "article_index = 0  # Change this to view different articles\n",
        "\n",
        "if len(articles) > article_index:\n",
        "    article = articles[article_index]\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(f\"TITLE: {article['title']}\")\n",
        "    print(f\"AUTHOR(S): {article['authors']}\")\n",
        "    print(f\"PUBLISHED: {article['publish_date']}\")\n",
        "    print(f\"URL: {article['resolved_url']}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(f\"\\nRAW CONTENT LENGTH: {article['raw_length']:,} characters\")\n",
        "    print(f\"CLEANED CONTENT LENGTH: {article['cleaned_length']:,} characters\")\n",
        "    print(f\"REMOVED: {article['raw_length'] - article['cleaned_length']:,} characters ({((article['raw_length'] - article['cleaned_length']) / article['raw_length'] * 100):.1f}% noise)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEFORE CLEANING (First 500 chars):\")\n",
        "    print(\"=\"*80)\n",
        "    print(article['raw_content'][:500] + \"...\\n\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "    print(\"AFTER CLEANING (Full cleaned article in paragraphs):\")\n",
        "    print(\"=\"*80)\n",
        "    print(article['cleaned_content'])\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "else:\n",
        "    print(f\"Article {article_index} not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Exported to news_articles_with_cleaning.csv\n",
            "✓ Exported to news_articles_with_cleaning.json\n",
            "✓ Exported 5 cleaned articles to news_articles_cleaned_only.csv\n"
          ]
        }
      ],
      "source": [
        "# Export with both raw and cleaned content\n",
        "df.to_csv(f'news_articles_with_cleaning_q={query}.csv', index=False, encoding='utf-8')\n",
        "print(\"✓ Exported to news_articles_with_cleaning.csv\")\n",
        "\n",
        "# Export JSON\n",
        "df.to_json(f'news_articles_with_cleaning_q={query}.json', orient='records', indent=2, force_ascii=False)\n",
        "print(\"✓ Exported to news_articles_with_cleaning.json\")\n",
        "\n",
        "# Export cleaned content only (for analysis)\n",
        "df_clean = df[df['extraction_status'] == 'Success'][['title', 'authors', 'publish_date', 'cleaned_content', 'keywords']]\n",
        "if len(df_clean) > 0:\n",
        "    df_clean.to_csv('news_articles_cleaned_only.csv', index=False, encoding='utf-8')\n",
        "    print(f\"✓ Exported {len(df_clean)} cleaned articles to news_articles_cleaned_only.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXTRACTION & CLEANING STATISTICS\n",
            "================================================================================\n",
            "Total articles: 5\n",
            "Successful extractions: 5\n",
            "Failed extractions: 0\n",
            "\n",
            "--- RAW CONTENT ---\n",
            "Average raw length: 28791 characters\n",
            "Total raw content: 143,953 characters\n",
            "\n",
            "--- CLEANED CONTENT ---\n",
            "Average cleaned length: 28057 characters\n",
            "Total cleaned content: 140,287 characters\n",
            "\n",
            "--- CLEANING EFFICIENCY ---\n",
            "Total noise removed: 3,666 characters\n",
            "Average noise percentage: 2.4%\n",
            "Shortest cleaned article: 3,824 characters\n",
            "Longest cleaned article: 74,278 characters\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Show cleaning statistics\n",
        "success_df = df[df['extraction_status'] == 'Success']\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTION & CLEANING STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total articles: {len(df)}\")\n",
        "print(f\"Successful extractions: {len(success_df)}\")\n",
        "print(f\"Failed extractions: {len(df) - len(success_df)}\")\n",
        "\n",
        "if len(success_df) > 0:\n",
        "    print(f\"\\n--- RAW CONTENT ---\")\n",
        "    print(f\"Average raw length: {success_df['raw_length'].mean():.0f} characters\")\n",
        "    print(f\"Total raw content: {success_df['raw_length'].sum():,} characters\")\n",
        "    \n",
        "    print(f\"\\n--- CLEANED CONTENT ---\")\n",
        "    print(f\"Average cleaned length: {success_df['cleaned_length'].mean():.0f} characters\")\n",
        "    print(f\"Total cleaned content: {success_df['cleaned_length'].sum():,} characters\")\n",
        "    \n",
        "    print(f\"\\n--- CLEANING EFFICIENCY ---\")\n",
        "    total_removed = success_df['raw_length'].sum() - success_df['cleaned_length'].sum()\n",
        "    avg_noise_pct = ((success_df['raw_length'] - success_df['cleaned_length']) / success_df['raw_length'] * 100).mean()\n",
        "    print(f\"Total noise removed: {total_removed:,} characters\")\n",
        "    print(f\"Average noise percentage: {avg_noise_pct:.1f}%\")\n",
        "    print(f\"Shortest cleaned article: {success_df['cleaned_length'].min():,} characters\")\n",
        "    print(f\"Longest cleaned article: {success_df['cleaned_length'].max():,} characters\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "multisource",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
